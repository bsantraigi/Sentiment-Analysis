{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 'os' for preliminary tasks like directory listing etc.\n",
    "import os\n",
    "\n",
    "# Import re for regex string matching\n",
    "import re\n",
    "\n",
    "# Import nltk for word tokenization\n",
    "import nltk\n",
    "\n",
    "# Import Python's native data structures Counter and defaultdict\n",
    "# Counter - maintains count of element\n",
    "# defaultdict - dictionary data structure with exception handling for missing keys\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Import tqdm for fancy progressbars!\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# Import numpy for different mathematical operations on arrays / matrices\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget   -P data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tar -xf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Samples\n",
    "- Dataset is split into two parts for training and testing\n",
    "- Positive and negative samples are organized in individual folders \n",
    "- Each sample document is stored in a .txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data/aclImdb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp = os.path.join(data_folder, 'train/pos')\n",
    "train_positive = [os.path.join(rp, f) for f in os.listdir(rp)]\n",
    "rp = os.path.join(data_folder, 'train/neg')\n",
    "train_negative = [os.path.join(rp, f) for f in os.listdir(rp)]\n",
    "\n",
    "rp = os.path.join(data_folder, 'test/pos')\n",
    "test_positive = [os.path.join(rp, f) for f in os.listdir(rp)]\n",
    "rp = os.path.join(data_folder, 'test/neg')\n",
    "test_negative = [os.path.join(rp, f) for f in os.listdir(rp)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "#### Probabilistic Model of Classification\n",
    "\n",
    "In a probabilistic classification model we want to estimate the value of $P(c | x)$, the probability of a sample x being of class c. Naive Bayes is one such probabilistic classifier that uses Bayes' Rule to classify samples. And Naive Bayes is _\"Naive\"_ because it assumes strong independence among all the features of sample x.\n",
    "\n",
    "#### Bayes Rule:\n",
    "\n",
    "$P(c|x) = \\frac{P(x|c)P(c)}{P(x)}$\n",
    "\n",
    "#### Text Classification using Naive Bayes classifier\n",
    "\n",
    "Consider the task of classifying textual documents into having positive or negative sentiments. We will design the Naive Bayes classifier for this problem as follows:\n",
    "\n",
    "Samples are text documents, and their features are the words that comprises these documents.\n",
    "\n",
    "- Each document $d$ is a sequence of words, $d = w_1w_2...w_n$, where $w_i$ are the tokens of the document and $n$ is the total number of tokens in the document $d$.\n",
    "\n",
    "- The training dataset consists of many document, sentiment pairs, ${d_i, s_i}$\n",
    "\n",
    "- Each document $d_i$ is associated with a sentiment $s_i \\in \\{0,1\\}$, $0$ being negative sentiment and $1$ being positive sentiment.\n",
    "\n",
    "Using Bayes' Rule we have \n",
    "\n",
    "- $p(s|doc) = \\frac{p(doc|s)p(s)}{p(doc|s)p(s) + p(doc|\\bar{s})p(\\bar{s})}$\n",
    "\n",
    "And from the independence assumption of features\n",
    "\n",
    "- $p(d|s) = p(w_1,w_2,..., w_n|s) = p(w_1|s)p(w_2|s)...p(w_n|s)$\n",
    "\n",
    "Also in the IMDb reviews dataset that we are considering here have equal number of positive and negative datasets.\n",
    "\n",
    "- We have $p(s) = 0.5$ and $p(\\bar{s})=0.5$.\n",
    "\n",
    "This simplifies our formulation for $p(s|d)$\n",
    "\n",
    "- $ p(s|d) = \\frac{p(d|s)}{p(d|s) + p(d|\\bar{s})} $\n",
    "\n",
    "- If we assign threshold of $p_T(s|d) = 0.5$ for deciding the final label, the model simplifies to,\n",
    " -  $y=1$ if $p(d|s=1) \\geq p(d|s=0)$.\n",
    "\n",
    "#### A measure for numerical stability\n",
    "\n",
    "$p(w_i)$ will be very small in magnitude, and when we take a product of such very small numbers to compute $p(d|s)$, even double precision floating points fail to store such small numbers and becomes zero. Hence, for numerical stability, we will convert the probabilities to log probability,\n",
    "\n",
    "$\\log p(d|s) = \\log p(w_1,w_2,..., w_n|s) = \\log p(w_1|s) + \\log p(w_2|s) + ...+ \\log p(w_n|s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex for cleaning html tags\n",
    "- Pattern <.*?> means \"anything within two angular brackets\". The qualifier *? denotes \"as few times as possible\". This makes sure we match only one html tag at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_html_cleaner = re.compile(r\"<.*?>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limit number of samples\n",
    "To quickly train a small model, consider setting n_train and n_test to some relatively small numbers e.g. `1000`. Set, \n",
    "`n_train = n_test = -1` to use all the samples available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 1000\n",
    "n_test = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Conditional) Unigram Counter\n",
    "- Calculates the distribution $p(w|s=1)$ and $p(w|s=0)$, empirically, from training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6e171d916b46de8d05aef7b0c6366f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Crunching +ve samples: ', max=1000, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8334e031ec423bb010844853d3d6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Crunching -ve samples: ', max=1000, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Distribution of word tokens in positive samples\n",
    "positive_word_counts = Counter()\n",
    "\n",
    "for _fname in tqdm_notebook(train_positive[:n_train], desc=\"Crunching +ve samples: \"):\n",
    "    with open(_fname) as f:\n",
    "        text = f.read().strip()\n",
    "        text = re_html_cleaner.sub(\" \", text)\n",
    "        positive_word_counts += Counter(nltk.word_tokenize(text))\n",
    "\n",
    "# Distribution of word tokens in negative samples\n",
    "negative_word_counts = Counter()\n",
    "\n",
    "for _fname in tqdm_notebook(train_negative[:n_train], desc=\"Crunching -ve samples: \"):\n",
    "    with open(_fname) as f:\n",
    "        text = f.read().strip()\n",
    "        text = re_html_cleaner.sub(\" \", text)\n",
    "        negative_word_counts += Counter(nltk.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram counts to probability distribution\n",
    "\n",
    "$p(w|s) = \\frac{N_{s,w}}{N_{s,*}} = \\frac{N_{s,w}}{\\sum_{w' \\in W}N_{s,w'}}$\n",
    "\n",
    "#### Additive Smoothing\n",
    "- Note that, if some token, $u$, unseen in training documents, occurrs in a test document, $p(doc_{test}|s)$ becomes $0$ as $N_{s,u}$ for that token is $0$.\n",
    "- We apply _Additive Smoothing_ to prevent probability from going to zero.\n",
    "\n",
    "$p(w|s) = \\frac{\\alpha + N_{s,w}}{\\sum_{w' \\in W}(\\alpha + N_{s,w'})} = \\frac{\\alpha + N_{s,w}}{\\alpha V + \\sum_{w' \\in W}N_{s,w'}}$\n",
    "\n",
    "where V is the total vocab size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_corpus_pos = sum(positive_word_counts.values())\n",
    "len_corpus_neg = sum(negative_word_counts.values())\n",
    "V_pos = len(positive_word_counts)\n",
    "V_neg = len(negative_word_counts)\n",
    "alpha = 0.1\n",
    "log_p_vocab_pos = defaultdict(\n",
    "    lambda: np.log(alpha/len_corpus_pos), \n",
    "    {w:np.log((alpha + c)/(V_pos*alpha + len_corpus_pos)) for w,c in positive_word_counts.items()}\n",
    ")\n",
    "log_p_vocab_neg = defaultdict(\n",
    "    lambda: np.log(alpha/len_corpus_neg), \n",
    "    {w:np.log((alpha + c)/(V_neg*alpha + len_corpus_neg)) for w,c in negative_word_counts.items()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prob. of +ve sentiment in our dataset: 0.5\n"
     ]
    }
   ],
   "source": [
    "p_data_pos = len(train_positive)/(len(train_positive) + len(train_negative))\n",
    "print(f\"Prob. of +ve sentiment in our dataset: {p_data_pos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_prob_pos(doc)\n",
    "\n",
    "A function that accepts a document string as input, tokenizes it and computes the probability $p(d|s=1)$ and $p(d|s=0)$. It returns 1 if $p(d|s=1) \\geq p(d|s=0)$ otherwise 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_pos(doc):\n",
    "    text = doc.strip()\n",
    "    text = re_html_cleaner.sub(\" \", text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    p_pos = 1\n",
    "    p_neg = 1\n",
    "    for token in tokens:\n",
    "        p_pos += log_p_vocab_pos[token]\n",
    "        p_neg += log_p_vocab_neg[token]\n",
    "        \n",
    "    return 1.0*(p_pos >= p_neg) #/(p_pos+p_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bef92b29f7d43c6afa15de684b8d7dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Classifying test data: ', max=1000, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e20cc36ce3402aaa12e7473170ab0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Classifying test data: ', max=1000, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for _fname in tqdm_notebook(test_positive[:n_test], desc=\"Classifying test data: \"):\n",
    "    with open(_fname) as f:\n",
    "        results.append((1, get_prob_pos(f.read())))\n",
    "        \n",
    "\n",
    "for _fname in tqdm_notebook(test_negative[:n_test], desc=\"Classifying test data: \"):\n",
    "    with open(_fname) as f:\n",
    "        results.append((0, get_prob_pos(f.read())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos = 0\n",
    "false_pos = 0\n",
    "true_neg = 0\n",
    "false_neg = 0\n",
    "for true_label, pred_label in results:\n",
    "    if true_label == 1 and pred_label == 1:\n",
    "        true_pos += 1\n",
    "    elif true_label == 1 and pred_label == 0:\n",
    "        false_neg += 1\n",
    "    elif true_label == 0 and pred_label == 1:\n",
    "        false_pos += 1\n",
    "    elif true_label == 0 and pred_label == 0:\n",
    "        true_neg += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation of our model\n",
    "\n",
    "**Accuracy:** Overall performance of our model, fraction of samples that were labelled correctly\n",
    "\n",
    "**Recall:** Out of all +ve data samples in test set, what fraction of it was labelled correctly\n",
    "\n",
    "**Precision:** How precise is the model? Out of all samples that were tagged +ve by the model, how many were actually positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7850\n",
      "Recall: 0.7410\n",
      "Precision: 0.8125\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {(true_pos + true_neg)/(true_pos + true_neg + false_pos + false_neg):0.4F}\")\n",
    "print(f\"Recall: {(true_pos)/(true_pos + false_neg):0.4F}\")\n",
    "print(f\"Precision: {(true_pos)/(true_pos + false_pos):0.4F}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
